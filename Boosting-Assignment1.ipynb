{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef139a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486f922",
   "metadata": {},
   "source": [
    "#### Boosting is another ensemble technique to create a collection of models. In this technique, models are learned sequentially with early models fitting simple models to the data and then analyzing the data for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e372ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d25c06",
   "metadata": {},
   "source": [
    "#### Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07646b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df740755",
   "metadata": {},
   "source": [
    "#### Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d186b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca60f37",
   "metadata": {},
   "source": [
    "#### different types of boosting algorithms\n",
    "\n",
    "-- ADA BOOSTING\n",
    "\n",
    "-- Gradient BOOSTING\n",
    "\n",
    "-- XG BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef792b",
   "metadata": {},
   "source": [
    "### some common parameters in boosting algorithms\n",
    "\n",
    "-- number of estimators\n",
    "\n",
    "-- learning Rate\n",
    "\n",
    "-- number of trees\n",
    "\n",
    "-- maximum depth of each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6410036f",
   "metadata": {},
   "source": [
    "#### Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d541c25c",
   "metadata": {},
   "source": [
    "#### steps involved in AdaBoost algorithm\n",
    "\n",
    "Step 1 – Creating the First Base Learner\n",
    "\n",
    "Step 2 – Calculating the Total Error (TE)\n",
    "\n",
    "Step 3 – Calculating Performance of the Stump\n",
    "\n",
    "Step 4 – Updating Weights\n",
    "\n",
    "Step 5 – Creating a New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c8c99",
   "metadata": {},
   "source": [
    "#### The error function that AdaBoost uses is an exponential loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934461fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f08e29",
   "metadata": {},
   "source": [
    "#### This is done by making misclassified cases to be updated with increased weights after an iteration. Increased weights would make our learning algorithm pay higher attention to these observations in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a917346",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6442a2b",
   "metadata": {},
   "source": [
    "### number of estimators are numbner of weak learners in ADA BOOST\n",
    "\n",
    "-- Accuracy will increase with nuymber of estimators/weak learners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
